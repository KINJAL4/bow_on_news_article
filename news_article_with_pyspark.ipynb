{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"Converting articles into BoW Vectors\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Kinjal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Converting articles into BoW Vectors</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2405b4bde80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.csv(\"all_news.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: string, Unnamed: 0: string, date: string, year: string, month: string, day: string, author: string, title: string, article: string, url: string, section: string, publication: string]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, regexp_replace,concat,lit,split,explode,regexp_extract\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import filterfalse\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select('title','article')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|             article|\n",
      "+--------------------+--------------------+\n",
      "|We should take co...|\"This post is par...|\n",
      "|Colts GM Ryan Gri...| The Indianapolis...|\n",
      "|                null|                null|\n",
      "|Trump denies repo...|DAVOS, Switzerlan...|\n",
      "|France's Sarkozy ...|PARIS (Reuters) -...|\n",
      "|Paris Hilton: Wom...|\"Paris Hilton arr...|\n",
      "|ECB's Coeure: If ...|BERLIN, June 17 (...|\n",
      "|                null|                null|\n",
      "|Venezuela detains...|CARACAS (Reuters)...|\n",
      "|You Can Trick You...|\"If only every da...|\n",
      "|How to watch the ...|Google I/O, the c...|\n",
      "|China is dismissi...|China is dismissi...|\n",
      "|“Elizabeth Warren...|Elizabeth Warren ...|\n",
      "|Hudson's Bay's ch...|(Reuters) - The s...|\n",
      "|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|\n",
      "|Jermaine Jackson ...|\"Jermaine Jackson...|\n",
      "|UK PM May presses...|LONDON (Reuters) ...|\n",
      "|Nancy Pelosi says...|\"Nancy Pelosi is ...|\n",
      "|The government of...|The nonpartisan d...|\n",
      "|Mark Zuckerberg’s...|The threat of gov...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.withColumn('text', F.concat(F.col('title'), F.col('article'))).drop(*data.columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|We should take co...|\n",
      "|Colts GM Ryan Gri...|\n",
      "|                null|\n",
      "|Trump denies repo...|\n",
      "|France's Sarkozy ...|\n",
      "|Paris Hilton: Wom...|\n",
      "|ECB's Coeure: If ...|\n",
      "|                null|\n",
      "|Venezuela detains...|\n",
      "|You Can Trick You...|\n",
      "|How to watch the ...|\n",
      "|China is dismissi...|\n",
      "|“Elizabeth Warren...|\n",
      "|Hudson's Bay's ch...|\n",
      "|Joakim Noah's Vic...|\n",
      "|Jermaine Jackson ...|\n",
      "|UK PM May presses...|\n",
      "|Nancy Pelosi says...|\n",
      "|The government of...|\n",
      "|Mark Zuckerberg’s...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_data = new_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2676886"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_null_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|We should take co...|\n",
      "|Colts GM Ryan Gri...|\n",
      "|Trump denies repo...|\n",
      "|France's Sarkozy ...|\n",
      "|Paris Hilton: Wom...|\n",
      "|ECB's Coeure: If ...|\n",
      "|Venezuela detains...|\n",
      "|You Can Trick You...|\n",
      "|How to watch the ...|\n",
      "|China is dismissi...|\n",
      "|“Elizabeth Warren...|\n",
      "|Hudson's Bay's ch...|\n",
      "|Joakim Noah's Vic...|\n",
      "|Jermaine Jackson ...|\n",
      "|UK PM May presses...|\n",
      "|Nancy Pelosi says...|\n",
      "|The government of...|\n",
      "|Mark Zuckerberg’s...|\n",
      "|Girl Scouts Are T...|\n",
      "|An Animated Maste...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "non_null_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = non_null_data.select(lower(col(\"text\")).alias(\"normalized tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   normalized tokens|\n",
      "+--------------------+\n",
      "|we should take co...|\n",
      "|colts gm ryan gri...|\n",
      "|trump denies repo...|\n",
      "|france's sarkozy ...|\n",
      "|paris hilton: wom...|\n",
      "|ecb's coeure: if ...|\n",
      "|venezuela detains...|\n",
      "|you can trick you...|\n",
      "|how to watch the ...|\n",
      "|china is dismissi...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalized_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(inputCol='normalized tokens',outputCol='tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_token = tokenizer.transform(normalized_data).select('words_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='normalized tokens', outputCol='words_token')\n",
    "data_words_token = tokenizer.transform(normalized_data).select('words_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "text = remover.transform(data_words_token).select('words_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         words_token|\n",
      "+--------------------+\n",
      "|[we, should, take...|\n",
      "|[colts, gm, ryan,...|\n",
      "|[trump, denies, r...|\n",
      "|[france's, sarkoz...|\n",
      "|[paris, hilton:, ...|\n",
      "|[ecb's, coeure:, ...|\n",
      "|[venezuela, detai...|\n",
      "|[you, can, trick,...|\n",
      "|[how, to, watch, ...|\n",
      "|[china, is, dismi...|\n",
      "|[“elizabeth, warr...|\n",
      "|[hudson's, bay's,...|\n",
      "|[joakim, noah's, ...|\n",
      "|[jermaine, jackso...|\n",
      "|[uk, pm, may, pre...|\n",
      "|[nancy, pelosi, s...|\n",
      "|[the, government,...|\n",
      "|[mark, zuckerberg...|\n",
      "|[girl, scouts, ar...|\n",
      "|[an, animated, ma...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_words_token.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "text = remover.transform(data_words_token).select('words_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                         words_clean|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|[take, concerns, health, liberal, democracy, seriously\"this, post, part, polyarchy,, independent,...|\n",
      "|[colts, gm, ryan, grigson, says, andrew, luck's, contract, makes, difficult, build, team, indiana...|\n",
      "|[trump, denies, report, ordered, mueller, fireddavos,, switzerland, (reuters), -, u.s., president...|\n",
      "|[france's, sarkozy, reveals, 'passions', insists, come-back, cardsparis, (reuters), -, former, fr...|\n",
      "|[paris, hilton:, woman, black, uncle, monty's, funeral\"paris, hilton, arrived, lax, wednesday, dr...|\n",
      "|[ecb's, coeure:, decide, cut, rates,, consider, tieringberlin,, june, 17, (reuters), -, ecb, boar...|\n",
      "|[venezuela, detains, six, military,, police, officials:, family, members,, activistscaracas, (reu...|\n",
      "|[trick, brain, focused\"if, every, day, like, this., can’t, put, finger, why:, maybe, right, amoun...|\n",
      "|[watch, google, i/o, keynote, livegoogle, i/o,, company's, big, developer, conference,, kicks, to...|\n",
      "|[china, dismissing, unfavorable, media, reports, fake, trump, doeschina, dismissing, unfavorable,...|\n",
      "|[“elizabeth, warren, called, me!”, turning, twitter, memeelizabeth, warren, giving, people, new, ...|\n",
      "|[hudson's, bay's, chairman's, buyout, bid, pits, retail, versus, real, estate(reuters), -, succes...|\n",
      "|[joakim, noah's, victoria, secret, model, gf, lais, ribeiro, rocks, thong, bikini, malibujoakim, ...|\n",
      "|[jermaine, jackson, rips, quincy, jones, scrubbing, michael's, name, show\"jermaine, jackson, ﻿is,...|\n",
      "|[uk, pm, may, presses, bid, get, brexit, deal, parliament:, spokesmanlondon, (reuters), -, britis...|\n",
      "|[nancy, pelosi, says, clinton, landslide, put, house, play., math, backs, up.\"nancy, pelosi, gett...|\n",
      "|[government, official, charge, ethics, harshly, condemned, trump’s, planthe, nonpartisan, directo...|\n",
      "|[mark, zuckerberg’s, wsj, op-ed, message, would-be, regulators:, hands, ad, businessthe, threat, ...|\n",
      "|[girl, scouts, training, next, generation, female, hackers\", past,, girl, scouts, rewarded, badge...|\n",
      "|[animated, masterpiece, unsung, co-founder, studio, ghiblibefore, 2016,, isao, takahata’s, yester...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-24-0293f77be1bc>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-0293f77be1bc>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    .setInputCols([“token”]) \\\u001b[0m\n\u001b[1;37m                              \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "stemmer = Stemmer().setInputCols([“token”]).setOutputCol(“stem”)\n",
    "stemmer.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens],T.ArrayType(T.StringType()))\n",
    "data_stemmed = text.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\")).select('words_stemmed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) >= 3], T.ArrayType(T.StringType()))\n",
    "data_final_words = data_stemmed.withColumn('words', filter_length_udf(col('words_stemmed')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data_final_words.select(explode(col(\"words_stemmed\")).alias(\"tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_groups = tokens.groupby(col(\"tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x1ea29e02c10>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = token_groups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o210.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 15.0 failed 1 times, most recent failure: Lost task 1.0 in stage 15.0 (TID 146) (Kinjal executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-16531c07d161>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtoken_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    438\u001b[0m         \"\"\"\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pyspark\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o210.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 15.0 failed 1 times, most recent failure: Lost task 1.0 in stage 15.0 (TID 146) (Kinjal executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\r\n\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\r\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\r\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\r\n\tat java.base/java.io.FilterOutputStream.write(FilterOutputStream.java:108)\r\n\tat org.apache.spark.api.python.PythonRDD$.write$1(PythonRDD.scala:295)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1(PythonRDD.scala:307)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$writeIteratorToStream$1$adapted(PythonRDD.scala:307)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\r\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.writeIteratorToStream(PythonUDFRunner.scala:53)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:397)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:232)\r\n"
     ]
    }
   ],
   "source": [
    "token_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.orderBy(\"count\",ascending=False).show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
