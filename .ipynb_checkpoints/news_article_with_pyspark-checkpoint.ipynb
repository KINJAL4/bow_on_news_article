{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName(\"Converting articles into BoW Vectors\").getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Kinjal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Converting articles into BoW Vectors</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1940f460ee0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=spark.read.csv(\"all_news.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_c0: string, Unnamed: 0: string, date: string, year: string, month: string, day: string, author: string, title: string, article: string, url: string, section: string, publication: string]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col, lower, regexp_replace,concat,lit,split,explode\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import filterfalse\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.select('title','article')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|             article|\n",
      "+--------------------+--------------------+\n",
      "|We should take co...|\"This post is par...|\n",
      "|Colts GM Ryan Gri...| The Indianapolis...|\n",
      "|                null|                null|\n",
      "|Trump denies repo...|DAVOS, Switzerlan...|\n",
      "|France's Sarkozy ...|PARIS (Reuters) -...|\n",
      "|Paris Hilton: Wom...|\"Paris Hilton arr...|\n",
      "|ECB's Coeure: If ...|BERLIN, June 17 (...|\n",
      "|                null|                null|\n",
      "|Venezuela detains...|CARACAS (Reuters)...|\n",
      "|You Can Trick You...|\"If only every da...|\n",
      "|How to watch the ...|Google I/O, the c...|\n",
      "|China is dismissi...|China is dismissi...|\n",
      "|“Elizabeth Warren...|Elizabeth Warren ...|\n",
      "|Hudson's Bay's ch...|(Reuters) - The s...|\n",
      "|Joakim Noah's Vic...|Joakim Noah's ﻿mo...|\n",
      "|Jermaine Jackson ...|\"Jermaine Jackson...|\n",
      "|UK PM May presses...|LONDON (Reuters) ...|\n",
      "|Nancy Pelosi says...|\"Nancy Pelosi is ...|\n",
      "|The government of...|The nonpartisan d...|\n",
      "|Mark Zuckerberg’s...|The threat of gov...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data.withColumn('text', F.concat(F.col('title'), F.col('article'))).drop(*data.columns[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|We should take co...|\n",
      "|Colts GM Ryan Gri...|\n",
      "|                null|\n",
      "|Trump denies repo...|\n",
      "|France's Sarkozy ...|\n",
      "|Paris Hilton: Wom...|\n",
      "|ECB's Coeure: If ...|\n",
      "|                null|\n",
      "|Venezuela detains...|\n",
      "|You Can Trick You...|\n",
      "|How to watch the ...|\n",
      "|China is dismissi...|\n",
      "|“Elizabeth Warren...|\n",
      "|Hudson's Bay's ch...|\n",
      "|Joakim Noah's Vic...|\n",
      "|Jermaine Jackson ...|\n",
      "|UK PM May presses...|\n",
      "|Nancy Pelosi says...|\n",
      "|The government of...|\n",
      "|Mark Zuckerberg’s...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_data = new_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = non_null_data.select(lower(col(\"text\")).alias(\"normalized tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preproc(x):\n",
    "    x = x.lower() #lower the text character\n",
    "    x = x.replace(\"'\",\"\").replace('\"', '')\n",
    "    x = x.encode('ascii', 'ignore').decode() #remove unicode characters\n",
    "    x = re.sub(r'https*\\S+', ' ', x) #remove urls\n",
    "    x = re.sub(r'@\\S+', ' ', x) # remove mentions\n",
    "    x = re.sub(r'#\\S+', ' ', x) # remove hashtags\n",
    "    x = re.sub(r'\\'\\w+', '', x) # remove ticks and the next chracter\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)# remove punctuations\n",
    "    x = re.sub('[^a-zA-Z]',' ',x) #remove non alphabetic characters\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x) # remove numbers\n",
    "    x = re.sub(r'\\s{2,}', ' ', x) # replace the overspaces\n",
    "    x = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ',x) #remove url\n",
    "    x = re.sub('[\\n]',' ',x) #remove newline character\n",
    "   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean_udf =F.udf(text_preproc, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text=normalized_data.select(data_clean_udf(col(\"normalized_tokens\")).alias(\"clean_text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='clean_text', outputCol='words_token')\n",
    "data_words_token = tokenizer.transform(clean_text).select('words_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_token.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "text = remover.transform(data_words_token).select('words_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "data_stemmed = text.withColumn(\"words_stemmed\", stemmer_udf(\"words_clean\")).select('words_stemmed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length_udf = udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
    "data_final_words = data_stemmed.withColumn('words', filter_length_udf(col('words_stemmed')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data_final_words.select(explode(col(\"words_stemmed\")).alias(\"tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_groups = tokens.groupby(col(\"tokens\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = token_groups.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts.orderBy(\"count\",ascending=False).show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
